Задание 1: Модификация существующих моделей

Выполнил модификацию моделей. Дополнительно для Logistic Regression добавил поддержку валидационной выборки т.к. модель очевидно сильно переобучалась. Пример запуска обучения моделей в homework_model_modification.py

Задание 2: Работа с датасетами

Создал класс для датасета Iris с атрибутом featureEngineering (используется опционально)
Обучал LinearRegression на датасете diabetes, LogisticRegression на датасете breast_cancer и Iris (с featureEngineering и без)

На датасете Iris при числе эпах более 20 точность предсказаний (acc) достигает 1 (но тестовая выборка очень мала)

Задание 3: Эксперименты и анализ

реализовал Feature Engineering для Iris. поэтому в ходе эксперимента с перебором параметров (batch size, learning rate, optimazer) построил диаграммы, которые показывают как изменение этих параметров в среднем влияет на Accuracy

Сначала приведу полную таблицу результатов для Iris при featureEngineering=False (10 эпох)

### Результаты экспериментов (feature_engineering=False)

| Optimizer | Learning Rate | Batch Size | Val Loss | Val Accuracy | Val F1 Score |
|-----------|----------------|-------------|----------|---------------|---------------|
| SGD       | 0.01           | 16          | 0.57     | 0.89          | 0.87          |
| SGD       | 0.01           | 32          | 0.83     | 0.76          | 0.70          |
| SGD       | 0.01           | 64          | 0.71     | 0.69          | 0.62          |
| SGD       | 0.05           | 16          | 0.36     | 0.87          | 0.84          |
| SGD       | 0.05           | 32          | 0.44     | 0.87          | 0.84          |
| SGD       | 0.05           | 64          | 0.54     | 0.80          | 0.74          |
| SGD       | 0.10           | 16          | 0.29     | 0.91          | 0.89          |
| SGD       | 0.10           | 32          | 0.35     | 0.89          | 0.87          |
| SGD       | 0.10           | 64          | 0.38     | 0.84          | 0.81          |
| Adam      | 0.01           | 16          | 0.30     | 0.87          | 0.84          |
| Adam      | 0.01           | 32          | 0.41     | 0.87          | 0.84          |
| Adam      | 0.01           | 64          | 0.65     | 0.80          | 0.74          |
| Adam      | 0.05           | 16          | 0.07     | 1.00          | 1.00          |
| Adam      | 0.05           | 32          | 0.18     | 0.96          | 0.95          |
| Adam      | 0.05           | 64          | 0.25     | 0.91          | 0.89          |
| Adam      | 0.10           | 16          | 0.03     | 1.00          | 1.00          |
| Adam      | 0.10           | 32          | 0.05     | 1.00          | 1.00          |
| Adam      | 0.10           | 64          | 0.09     | 0.98          | 0.97          |
| RMSprop   | 0.01           | 16          | 0.24     | 0.93          | 0.92          |
| RMSprop   | 0.01           | 32          | 0.23     | 0.96          | 0.95          |
| RMSprop   | 0.01           | 64          | 0.32     | 0.93          | 0.92          |
| RMSprop   | 0.05           | 16          | 0.04     | 1.00          | 1.00          |
| RMSprop   | 0.05           | 32          | 0.09     | 1.00          | 1.00          |
| RMSprop   | 0.05           | 64          | 0.08     | 1.00          | 1.00          |
| RMSprop   | 0.10           | 16          | 0.02     | 1.00          | 1.00          |
| RMSprop   | 0.10           | 32          | 0.04     | 1.00          | 1.00          |
| RMSprop   | 0.10           | 64          | 0.04     | 1.00          | 1.00          |




### Та же таблица при featureEngineering=True

| Optimizer | Learning Rate | Batch Size | Val Loss | Val Accuracy | Val F1 Score |
|-----------|---------------|------------|----------|--------------|--------------|
| SGD       | 0.01          | 16         | 0.38     | 0.87         | 0.84         |
| SGD       | 0.01          | 32         | 0.52     | 0.82         | 0.77         |
| SGD       | 0.01          | 64         | 0.49     | 0.96         | 0.95         |
| SGD       | 0.05          | 16         | 0.19     | 0.96         | 0.95         |
| SGD       | 0.05          | 32         | 0.31     | 0.87         | 0.84         |
| SGD       | 0.05          | 64         | 0.32     | 0.89         | 0.87         |
| SGD       | 0.10          | 16         | 0.13     | 0.98         | 0.97         |
| SGD       | 0.10          | 32         | 0.19     | 0.96         | 0.95         |
| SGD       | 0.10          | 64         | 0.23     | 0.93         | 0.92         |
| Adam      | 0.01          | 16         | 0.14     | 0.98         | 0.97         |
| Adam      | 0.01          | 32         | 0.17     | 0.98         | 0.97         |
| Adam      | 0.01          | 64         | 0.25     | 0.93         | 0.92         |
| Adam      | 0.05          | 16         | 0.01     | 1.00         | 1.00         |
| Adam      | 0.05          | 32         | 0.02     | 1.00         | 1.00         |
| Adam      | 0.05          | 64         | 0.05     | 1.00         | 1.00         |
| Adam      | 0.10          | 16         | 0.00     | 1.00         | 1.00         |
| Adam      | 0.10          | 32         | 0.01     | 1.00         | 1.00         |
| Adam      | 0.10          | 64         | 0.02     | 1.00         | 1.00         |
| RMSprop   | 0.01          | 16         | 0.05     | 1.00         | 1.00         |
| RMSprop   | 0.01          | 32         | 0.08     | 1.00         | 1.00         |
| RMSprop   | 0.01          | 64         | 0.11     | 1.00         | 1.00         |
| RMSprop   | 0.05          | 16         | 0.01     | 1.00         | 1.00         |
| RMSprop   | 0.05          | 32         | 0.02     | 1.00         | 1.00         |
| RMSprop   | 0.05          | 64         | 0.02     | 1.00         | 1.00         |
| RMSprop   | 0.10          | 16         | 0.00     | 1.00         | 1.00         |
| RMSprop   | 0.10          | 32         | 0.00     | 1.00         | 1.00         |
| RMSprop   | 0.10          | 64         | 0.00     | 1.00         | 1.00         |



Видно, что для Adam и SGD получилась ощутимая прибавка в качестве. Особенно для SGD

featureEngineering=True
![feature_engineering_val_acc_vs_optimizer](https://github.com/user-attachments/assets/96f109af-ff4a-4330-a165-79e96e83da37)
featureEngineering=False
![val_acc_vs_optimizer](https://github.com/user-attachments/assets/827f2e3b-12b4-40ec-ba91-ee3f7f6e2f90)


Также я построил другие графики.
У меня получилось так, что при увеличении батча качество снижается, а при увеличении learning rate качество возрастает. Причем независимо от того, был ли featureEngineering






